import sys
import pathlib
import json
import traceback
import re
import tiktoken
import httpx # Add httpx for sending updates to GUI

# Load Config
sys.path.append(str(pathlib.Path(__file__).parent.parent.parent.parent / "config"))
from config import Config

config = Config()

def extract_json_from_text(content):
    json_match = re.search(r"```json\s*(.*?)\s*```", content, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    print("[ERROR] No JSON block detected in OpenAI response. Returning raw content.")
    return content.strip()  # fallback to raw content

def fix_json_format(raw_response):
    try:
        return json.loads(raw_response)
    except json.JSONDecodeError:
        print("[WARNING] Attempting to fix JSON format...")
        cleaned_response = re.sub(r'[^a-zA-Z0-9:{}\[\],"\'.\s]', '', raw_response)
        cleaned_response = re.sub(r',?\s*{}\s*', '', cleaned_response)
        cleaned_response = re.sub(r'"done"\s*:\s*{(.*?)\s*}', r'"done": {"summary": "\1"}', cleaned_response)
        cleaned_response = re.sub(r'([{,])\s*([a-zA-Z0-9_]+)\s*:', r'\1 "\2":', cleaned_response)
        try:
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSON Decode Failed After Fixing: {e}")
            return []

def count_tokens(messages, model_name):
    try:
        enc = tiktoken.encoding_for_model(model_name)
    except KeyError:
        enc = tiktoken.get_encoding("cl100k_base")
    return sum(len(enc.encode(msg["content"])) for msg in messages)

def truncate_messages(messages, max_tokens, model_name):
    while count_tokens(messages, model_name) > max_tokens and len(messages) > 1:
        messages.pop(1)  # remove oldest user message first
    return messages

def transform_operations(response_list):
    transformed = []
    for item in response_list:
        if isinstance(item, dict) and "operation" in item:
            transformed.append(item)
        else:
            print(f"[WARNING] Unexpected operation format: {item}")
    return transformed

async def _update_gui_with_stream_chunk(chunk_text: str):
    """Helper to send a chunk of streamed LLM output to the GUI."""
    try:
        # Assuming a specific endpoint for LLM stream updates, e.g., /state/llm_stream
        url = "http://127.0.0.1:8000/state/llm_stream_chunk" 
        async with httpx.AsyncClient() as client:
            await client.post(url, json={"chunk": chunk_text}, timeout=2)
    except Exception as e:
        # Avoid flooding logs if GUI is not responsive or endpoint is wrong
        # print(f"[STREAM_GUI_UPDATE_ERROR] Failed to send chunk to GUI: {e}")
        pass

async def call_openai_model(messages, objective, model_name=None):
    try:
        model_name = model_name or config.get_model()
        temperature = config.get_temperature()
        
        # Always print messages for objective formulation to debug
        if "formulate" in str(messages).lower() or "objective" in str(messages).lower():
            print(f"[OBJECTIVE_DEBUG] ******* OPENAI HANDLER CALLED FOR OBJECTIVE FORMULATION *******")
            print(f"[OBJECTIVE_DEBUG] Model: {model_name}")
            print(f"[OBJECTIVE_DEBUG] Temperature: {temperature}")
            print(f"[OBJECTIVE_DEBUG] Raw messages: {json.dumps(messages, indent=2)}")
            print(f"[OBJECTIVE_DEBUG] Objective arg: {objective}")
        
        if config.get("DEBUG", False):
            print(f"[DEBUG] Raw messages:\\n{json.dumps(messages, indent=2)}")

        content = messages[-1]["content"] if messages and "content" in messages[-1] else ""

        def truncate_text(text, max_tokens, model_name):
            try:
                enc = tiktoken.encoding_for_model(model_name)
            except KeyError:
                enc = tiktoken.get_encoding("cl100k_base")
            tokenized = enc.encode(text)
            return enc.decode(tokenized[:max_tokens]) if len(tokenized) > max_tokens else text        content = truncate_text(content, max_tokens=2000, model_name=model_name)
        truncated_data = truncate_messages([{"role": "system", "content": content}], max_tokens=6000, model_name=model_name)
        messages = messages[:-1] + truncated_data
        
        if config.get("DEBUG", False):
            print(f"[DEBUG] Truncated messages:\n{json.dumps(messages, indent=2)}")
            
        import openai
        
        # Check if API key is available
        api_key = config.get("OPENAI_API_KEY")
        if not api_key:
            print("[CRITICAL ERROR] No OpenAI API key found! Cannot call LLM API.")
            return "ERROR: No OpenAI API key configured. Please add your API key to config."
            
        print(f"[API_DEBUG] Using API key: {api_key[:5]}...{api_key[-4:] if len(api_key) > 8 else ''}")
        client = openai.OpenAI(api_key=api_key)

        stream = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
            presence_penalty=1,
            frequency_penalty=1,
            stream=True,
        )

        collected_content = ""
        # Clear previous stream content on GUI if applicable
        await _update_gui_with_stream_chunk("__STREAM_START__") # Signal stream start

        for chunk in stream:
            delta = getattr(chunk.choices[0].delta, "content", None)
            if delta:
                if config.get("DEBUG", False):
                    print(delta, end="", flush=True)
                collected_content += delta
                await _update_gui_with_stream_chunk(delta) # Send chunk to GUI        await _update_gui_with_stream_chunk("__STREAM_END__") # Signal stream end
        
        if config.get("DEBUG", False):
            print(f"\n[DEBUG] Raw GPT Response Content (streamed):\n{repr(collected_content)}")
            
        # Always print response for objective formulation
        if "formulate" in str(messages).lower() or "objective" in str(messages).lower():
            print(f"\n[OBJECTIVE_DEBUG] ******* LLM RESPONSE FOR OBJECTIVE FORMULATION *******")
            print(f"[OBJECTIVE_DEBUG] Raw response: {repr(collected_content)}")
            print(f"[OBJECTIVE_DEBUG] Response length: {len(collected_content)}")

        raw_json_text = extract_json_from_text(collected_content)
        
        # If extract_json_from_text returns the raw content because no JSON block was found,
        # we should not try to parse it as JSON.
        # Instead, the raw text (which is `collected_content` in this case) should be returned.
        # The `get_next_action` in `lm_interface` expects a string response for visual/thinking/steps,
        # and a JSON string (that it will parse) for actions.

        # Let's adjust the return. If a JSON block is found and extracted, `raw_json_text` will be that.
        # If not, `raw_json_text` will be `collected_content`.
        # The `handle_llm_response` function (or other callers) will attempt to parse it.
        # For non-action stages (visual, thinking, steps), they expect raw text.
        # For action stage, it expects JSON.

        # The `transform_operations` and `fix_json_format` are specifically for action JSON.
        # We should only apply them if we expect a JSON action.
        # The `objective` parameter might give a hint, or we can rely on the prompt structure.

        # For now, let's assume if a JSON block is found, it's an action.
        # Otherwise, it's a text response for other stages.
        
        json_match = re.search(r"```json\\s*(.*?)\\s*```", collected_content, re.DOTALL)
        if json_match:
            # It's likely an action response
            parsed_json_actions = fix_json_format(raw_json_text) # raw_json_text is the extracted JSON part
            transformed_actions = transform_operations(parsed_json_actions if isinstance(parsed_json_actions, list) else [parsed_json_actions])
            # Return the JSON string of the transformed actions
            return json.dumps(transformed_actions) if transformed_actions else "[]" # Ensure it's a JSON string
        else:
            # It's likely a text response (visual analysis, thinking, steps)
            return collected_content.strip()


    except Exception as e:
        print(f"[ERROR] Exception in call_openai_model: {e}")
        if config.get("DEBUG", False):
            traceback.print_exc()
        return []
